{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from methods.model_fitting_utilities import softmax_neg_log_likelihood\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv('./data/model_fitting_outputs/summary_data.csv', sep=';')\n",
    "final_judgements = pd.read_csv('./data/model_fitting_outputs/final_judgements.csv')\n",
    "summary['information_gained'] = (summary['prior_entropy'] - summary['posterior_entropy']) / summary['prior_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final judgements shape: (15625, 1092)\n",
      "normative posteriors shape: (15625, 1092)\n",
      "LC_discrete posteriors shape: (15625, 1092)\n",
      "LC_discrete_attention posteriors shape: (15625, 1092)\n",
      "change_discrete posteriors shape: (15625, 1092)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Final judgements shape: {final_judgements.shape}')\n",
    "posteriors = {}\n",
    "\n",
    "for model_name in summary.model_name.unique():\n",
    "    posteriors[model_name] = pd.read_csv(f'./data/model_fitting_outputs/{model_name}/posteriors.csv')\n",
    "    print(f'{model_name} posteriors shape: {posteriors[model_name].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoints per experiment:\n",
      "Total: N=302 length=4344\n",
      "Experiment 2: N=60 length=480\n",
      "Experiment 2: N=121 length=1936\n",
      "Experiment 3: N=121 length=1928\n"
     ]
    }
   ],
   "source": [
    "print('Datapoints per experiment:')\n",
    "print('Total:', f'N={summary.pid.nunique()}', f'length={summary.shape[0]}')\n",
    "print('Experiment 2:', f'N={summary[summary.experiment == \"experiment_1\"].pid.nunique()}', f'length={summary[summary.experiment == \"experiment_1\"].shape[0]}')\n",
    "print('Experiment 2:', f'N={summary[summary.experiment == \"experiment_2\"].pid.nunique()}', f'length={summary[summary.experiment == \"experiment_2\"].shape[0]}')\n",
    "print('Experiment 3:', f'N={summary[summary.experiment == \"experiment_3\"].pid.nunique()}', f'length={summary[summary.experiment == \"experiment_3\"].shape[0]}')\n",
    "\n",
    "# Split data into 3 dataset\n",
    "summary_1 = summary[summary.experiment == \"experiment_1\"]\n",
    "summary_2 = summary[summary.experiment == \"experiment_2\"]\n",
    "summary_3 = summary[summary.experiment == \"experiment_3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utid</th>\n",
       "      <th>pid</th>\n",
       "      <th>experiment</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>scenario</th>\n",
       "      <th>model_name</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>posterior_map</th>\n",
       "      <th>posterior_judgement</th>\n",
       "      <th>prior_judgement</th>\n",
       "      <th>prior_entropy</th>\n",
       "      <th>posterior_entropy</th>\n",
       "      <th>model_specs</th>\n",
       "      <th>information_gained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3_566feba6b937e400052d33b2_finance_congruent</td>\n",
       "      <td>566feba6b937e400052d33b2</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>congruent</td>\n",
       "      <td>finance</td>\n",
       "      <td>normative</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 0.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>9.390294</td>\n",
       "      <td>7.981602e-36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3_566feba6b937e400052d33b2_finance_congruent</td>\n",
       "      <td>566feba6b937e400052d33b2</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>congruent</td>\n",
       "      <td>finance</td>\n",
       "      <td>LC_discrete</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[1.  0.5 0.  0.  0.5 1. ]</td>\n",
       "      <td>[ 0.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>9.390294</td>\n",
       "      <td>6.192261e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3_566feba6b937e400052d33b2_finance_congruent</td>\n",
       "      <td>566feba6b937e400052d33b2</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>congruent</td>\n",
       "      <td>finance</td>\n",
       "      <td>LC_discrete_attention</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 0.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 1.   1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>9.390294</td>\n",
       "      <td>9.390294e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_566feba6b937e400052d33b2_dampened_generic</td>\n",
       "      <td>566feba6b937e400052d33b2</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>generic</td>\n",
       "      <td>dampened</td>\n",
       "      <td>normative</td>\n",
       "      <td>[-1.   0.5  0.   1.   0.   0. ]</td>\n",
       "      <td>[-1.   0.5  0.   1.   0.   0. ]</td>\n",
       "      <td>[0.5 0.  0.5 0.  0.  0. ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.931326</td>\n",
       "      <td>9.939385e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.928654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_566feba6b937e400052d33b2_dampened_generic</td>\n",
       "      <td>566feba6b937e400052d33b2</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>generic</td>\n",
       "      <td>dampened</td>\n",
       "      <td>LC_discrete</td>\n",
       "      <td>[-1.   0.5  0.   1.   0.   0. ]</td>\n",
       "      <td>[-1. -1.  0.  1.  0.  0.]</td>\n",
       "      <td>[0.5 0.  0.5 0.  0.  0. ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.931326</td>\n",
       "      <td>6.574633e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>3_61717173748006894b2b54ff_neg_chain_generic_2</td>\n",
       "      <td>61717173748006894b2b54ff</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>generic_2</td>\n",
       "      <td>neg_chain</td>\n",
       "      <td>change_discrete</td>\n",
       "      <td>[-1  0  0 -1  0  0]</td>\n",
       "      <td>[-0.5 -1.   0.  -0.5  0.5 -0.5]</td>\n",
       "      <td>[-0.5  1.   0.  -0.5  0.   0. ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.931559</td>\n",
       "      <td>1.301234e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.906598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>3_6176966806de000024ed0ddf_crime_congruent</td>\n",
       "      <td>6176966806de000024ed0ddf</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>congruent</td>\n",
       "      <td>crime</td>\n",
       "      <td>change_discrete</td>\n",
       "      <td>[-1.   0.5  0.5  1.   0.   1. ]</td>\n",
       "      <td>[ 1.  1.  1.  1.  0. -1.]</td>\n",
       "      <td>[ 1.  1.  1.  0.  0. -1.]</td>\n",
       "      <td>[-1.   0.5  0.5  1.   0.   1. ]</td>\n",
       "      <td>13.728335</td>\n",
       "      <td>4.208897e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4341</th>\n",
       "      <td>3_6176966806de000024ed0ddf_dampened_generic</td>\n",
       "      <td>6176966806de000024ed0ddf</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>generic</td>\n",
       "      <td>dampened</td>\n",
       "      <td>change_discrete</td>\n",
       "      <td>[-1.  -0.5  0.  -1.   0.   0. ]</td>\n",
       "      <td>[ 0.  -1.   0.5 -1.   1.  -1. ]</td>\n",
       "      <td>[ 0.   0.  -0.5 -0.5  0.   0.5]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.931559</td>\n",
       "      <td>2.997061e-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>3_6176966806de000024ed0ddf_finance_incongruent</td>\n",
       "      <td>6176966806de000024ed0ddf</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>incongruent</td>\n",
       "      <td>finance</td>\n",
       "      <td>change_discrete</td>\n",
       "      <td>[ 0.  -0.5  0.5 -1.   1.   0. ]</td>\n",
       "      <td>[ 0.  -1.   1.  -0.5  1.   0. ]</td>\n",
       "      <td>[ 0.5 -1.  -0.5 -0.5  1.   1. ]</td>\n",
       "      <td>[ 0.5  1.   0.   0.  -0.5 -1. ]</td>\n",
       "      <td>13.753367</td>\n",
       "      <td>3.559470e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>3_6176966806de000024ed0ddf_neg_chain_generic_2</td>\n",
       "      <td>6176966806de000024ed0ddf</td>\n",
       "      <td>experiment_3</td>\n",
       "      <td>generic_2</td>\n",
       "      <td>neg_chain</td>\n",
       "      <td>change_discrete</td>\n",
       "      <td>[-1  0  0 -1  0  0]</td>\n",
       "      <td>[-1.   0.5  0.  -1.   0.   0. ]</td>\n",
       "      <td>[-1.   1.   1.  -0.5  0.  -1. ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.931559</td>\n",
       "      <td>4.853678e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.651606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1928 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                utid  \\\n",
       "0       3_566feba6b937e400052d33b2_finance_congruent   \n",
       "1       3_566feba6b937e400052d33b2_finance_congruent   \n",
       "2       3_566feba6b937e400052d33b2_finance_congruent   \n",
       "3        3_566feba6b937e400052d33b2_dampened_generic   \n",
       "4        3_566feba6b937e400052d33b2_dampened_generic   \n",
       "...                                              ...   \n",
       "4339  3_61717173748006894b2b54ff_neg_chain_generic_2   \n",
       "4340      3_6176966806de000024ed0ddf_crime_congruent   \n",
       "4341     3_6176966806de000024ed0ddf_dampened_generic   \n",
       "4342  3_6176966806de000024ed0ddf_finance_incongruent   \n",
       "4343  3_6176966806de000024ed0ddf_neg_chain_generic_2   \n",
       "\n",
       "                           pid    experiment   difficulty   scenario  \\\n",
       "0     566feba6b937e400052d33b2  experiment_3    congruent    finance   \n",
       "1     566feba6b937e400052d33b2  experiment_3    congruent    finance   \n",
       "2     566feba6b937e400052d33b2  experiment_3    congruent    finance   \n",
       "3     566feba6b937e400052d33b2  experiment_3      generic   dampened   \n",
       "4     566feba6b937e400052d33b2  experiment_3      generic   dampened   \n",
       "...                        ...           ...          ...        ...   \n",
       "4339  61717173748006894b2b54ff  experiment_3    generic_2  neg_chain   \n",
       "4340  6176966806de000024ed0ddf  experiment_3    congruent      crime   \n",
       "4341  6176966806de000024ed0ddf  experiment_3      generic   dampened   \n",
       "4342  6176966806de000024ed0ddf  experiment_3  incongruent    finance   \n",
       "4343  6176966806de000024ed0ddf  experiment_3    generic_2  neg_chain   \n",
       "\n",
       "                 model_name                     ground_truth  \\\n",
       "0                 normative  [ 1.   1.  -0.5 -0.5  1.   1. ]   \n",
       "1               LC_discrete  [ 1.   1.  -0.5 -0.5  1.   1. ]   \n",
       "2     LC_discrete_attention  [ 1.   1.  -0.5 -0.5  1.   1. ]   \n",
       "3                 normative  [-1.   0.5  0.   1.   0.   0. ]   \n",
       "4               LC_discrete  [-1.   0.5  0.   1.   0.   0. ]   \n",
       "...                     ...                              ...   \n",
       "4339        change_discrete              [-1  0  0 -1  0  0]   \n",
       "4340        change_discrete  [-1.   0.5  0.5  1.   0.   1. ]   \n",
       "4341        change_discrete  [-1.  -0.5  0.  -1.   0.   0. ]   \n",
       "4342        change_discrete  [ 0.  -0.5  0.5 -1.   1.   0. ]   \n",
       "4343        change_discrete              [-1  0  0 -1  0  0]   \n",
       "\n",
       "                        posterior_map              posterior_judgement  \\\n",
       "0     [ 1.   1.  -0.5 -0.5  1.   1. ]  [ 0.   1.  -0.5 -0.5  1.   1. ]   \n",
       "1           [1.  0.5 0.  0.  0.5 1. ]  [ 0.   1.  -0.5 -0.5  1.   1. ]   \n",
       "2     [ 1.   1.  -0.5 -0.5  1.   1. ]  [ 0.   1.  -0.5 -0.5  1.   1. ]   \n",
       "3     [-1.   0.5  0.   1.   0.   0. ]        [0.5 0.  0.5 0.  0.  0. ]   \n",
       "4           [-1. -1.  0.  1.  0.  0.]        [0.5 0.  0.5 0.  0.  0. ]   \n",
       "...                               ...                              ...   \n",
       "4339  [-0.5 -1.   0.  -0.5  0.5 -0.5]  [-0.5  1.   0.  -0.5  0.   0. ]   \n",
       "4340        [ 1.  1.  1.  1.  0. -1.]        [ 1.  1.  1.  0.  0. -1.]   \n",
       "4341  [ 0.  -1.   0.5 -1.   1.  -1. ]  [ 0.   0.  -0.5 -0.5  0.   0.5]   \n",
       "4342  [ 0.  -1.   1.  -0.5  1.   0. ]  [ 0.5 -1.  -0.5 -0.5  1.   1. ]   \n",
       "4343  [-1.   0.5  0.  -1.   0.   0. ]  [-1.   1.   1.  -0.5  0.  -1. ]   \n",
       "\n",
       "                      prior_judgement  prior_entropy  posterior_entropy  \\\n",
       "0     [ 1.   1.  -0.5 -0.5  1.   1. ]       9.390294       7.981602e-36   \n",
       "1     [ 1.   1.  -0.5 -0.5  1.   1. ]       9.390294       6.192261e-05   \n",
       "2     [ 1.   1.  -0.5 -0.5  1.   1. ]       9.390294       9.390294e+00   \n",
       "3                                 NaN      13.931326       9.939385e-01   \n",
       "4                                 NaN      13.931326       6.574633e-03   \n",
       "...                               ...            ...                ...   \n",
       "4339                              NaN      13.931559       1.301234e+00   \n",
       "4340  [-1.   0.5  0.5  1.   0.   1. ]      13.728335       4.208897e-01   \n",
       "4341                              NaN      13.931559       2.997061e-04   \n",
       "4342  [ 0.5  1.   0.   0.  -0.5 -1. ]      13.753367       3.559470e-03   \n",
       "4343                              NaN      13.931559       4.853678e+00   \n",
       "\n",
       "      model_specs  information_gained  \n",
       "0             NaN            1.000000  \n",
       "1             NaN            0.999993  \n",
       "2             NaN            0.000000  \n",
       "3             NaN            0.928654  \n",
       "4             NaN            0.999528  \n",
       "...           ...                 ...  \n",
       "4339          NaN            0.906598  \n",
       "4340          NaN            0.969342  \n",
       "4341          NaN            0.999978  \n",
       "4342          NaN            0.999741  \n",
       "4343          NaN            0.651606  \n",
       "\n",
       "[1928 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gained\n",
      "Normative\n",
      "Congruent: mean=0.9963, std=0.0243\n",
      "Incongruent: mean=0.992, std=0.0316\n",
      "Ttest_indResult(statistic=1.188732138968508, pvalue=0.2357298236523205)\n",
      "LC_discrete\n",
      "Congruent: mean=0.9953, std=0.0215\n",
      "Incongruent: mean=0.9915, std=0.025\n",
      "Ttest_indResult(statistic=1.188732138968508, pvalue=0.2357298236523205)\n",
      "LC_discrete_attention\n",
      "Congruent: mean=0.8817, std=0.27\n",
      "Incongruent: mean=0.9117, std=0.2214\n",
      "Ttest_indResult(statistic=-0.9414486846448392, pvalue=0.347429561901356)\n",
      "Discrete Change\n",
      "Congruent: mean=0.8789, std=0.2665\n",
      "Incongruent: mean=0.911, std=0.211\n",
      "Ttest_indResult(statistic=-1.0329441239542303, pvalue=0.30267852362438635)\n"
     ]
    }
   ],
   "source": [
    "congruent = summary_3[summary_3.difficulty == 'congruent']\n",
    "incongruent = summary_3[summary_3.difficulty == 'incongruent']\n",
    "\n",
    "congruent_norm = congruent[congruent.model_name == 'normative'].information_gained\n",
    "incongruent_norm = incongruent[incongruent.model_name == 'normative'].information_gained\n",
    "\n",
    "congruent_lc = congruent[congruent.model_name == 'LC_discrete'].information_gained\n",
    "incongruent_lc = incongruent[incongruent.model_name == 'LC_discrete'].information_gained\n",
    "\n",
    "congruent_lc_a = congruent[congruent.model_name == 'LC_discrete_attention'].information_gained\n",
    "incongruent_lc_a = incongruent[incongruent.model_name == 'LC_discrete_attention'].information_gained\n",
    "\n",
    "congruent_change = congruent[congruent.model_name == 'change_discrete'].information_gained\n",
    "incongruent_change = incongruent[incongruent.model_name == 'change_discrete'].information_gained\n",
    "\n",
    "print('Information gained')\n",
    "print('Normative')\n",
    "print(f'Congruent: mean={congruent_norm.mean().round(4)}, std={congruent_norm.std().round(4)}')\n",
    "print(f'Incongruent: mean={incongruent_norm.mean().round(4)}, std={incongruent_norm.std().round(4)}')\n",
    "print(stats.ttest_ind(congruent_norm, incongruent_norm))\n",
    "\n",
    "print('LC_discrete')\n",
    "print(f'Congruent: mean={congruent_lc.mean().round(4)}, std={congruent_lc.std().round(4)}')\n",
    "print(f'Incongruent: mean={incongruent_lc.mean().round(4)}, std={incongruent_lc.std().round(4)}')\n",
    "print(stats.ttest_ind(congruent_norm, incongruent_norm))\n",
    "\n",
    "print('LC_discrete_attention')\n",
    "print(f'Congruent: mean={congruent_lc_a.mean().round(4)}, std={congruent_lc_a.std().round(4)}')\n",
    "print(f'Incongruent: mean={incongruent_lc_a.mean().round(4)}, std={incongruent_lc_a.std().round(4)}')\n",
    "print(stats.ttest_ind(congruent_lc_a, incongruent_lc_a))\n",
    "\n",
    "print('Discrete Change')\n",
    "print(f'Congruent: mean={congruent_change.mean().round(4)}, std={congruent_change.std().round(4)}')\n",
    "print(f'Incongruent: mean={incongruent_change.mean().round(4)}, std={incongruent_change.std().round(4)}')\n",
    "print(stats.ttest_ind(congruent_change, incongruent_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only data not links\n",
    "\n",
    "judgements_arr = final_judgements[final_judgements.columns[6:]].to_numpy()\n",
    "\n",
    "# Recover posteriors for each modes\n",
    "posteriors_arr = {}\n",
    "for model_name in summary.model_name.unique():\n",
    "    posteriors_arr[model_name] = posteriors[model_name][posteriors[model_name].columns[6:]].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretenans = np.argwhere(np.isnan(posteriors_arr['LC_discrete']))\n",
    "temp=0\n",
    "\n",
    "nanidx = np.unique(discretenans[:, 1])\n",
    "\n",
    "model_names = ['normative', 'LC_discrete', 'LC_discrete_attention', 'change_discrete']\n",
    "nLL = np.zeros(len(model_names))\n",
    "optim_results = [None for i in model_names]\n",
    "for i, model in enumerate(model_names):\n",
    "    selection = np.delete(judgements_arr, nanidx, axis=1)\n",
    "    dataset = np.delete(posteriors_arr[model], nanidx, axis=1)\n",
    "    #nLL[i] = softmax_neg_log_likelihood(temp, dataset, judgements_arr)\n",
    "    optim_results[i] = minimize(softmax_neg_log_likelihood, \n",
    "                              0, \n",
    "                              args=(dataset, selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform negative log likelihood (baseline) = 10487.1 \n",
      "\n",
      "Model name: normative\n",
      "Negative Log Likelihood = 10446.786038761678\n",
      "Temperature = 4.493196817730923\n",
      "Optim. result: Optimization terminated successfully. \n",
      "\n",
      "Model name: LC_discrete\n",
      "Negative Log Likelihood = 10462.94930681456\n",
      "Temperature = 3.387430856637707\n",
      "Optim. result: Optimization terminated successfully. \n",
      "\n",
      "Model name: LC_discrete_attention\n",
      "Negative Log Likelihood = 10457.622909812366\n",
      "Temperature = 4.119688694925813\n",
      "Optim. result: Optimization terminated successfully. \n",
      "\n",
      "Model name: change_discrete\n",
      "Negative Log Likelihood = 10159.79672505864\n",
      "Temperature = 7.107513697001946\n",
      "Optim. result: Optimization terminated successfully. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Uniform negative log likelihood (baseline) = 10487.1 \\n')\n",
    "for i, model in enumerate(model_names):\n",
    "    print(f'Model name: {model}')\n",
    "    print(f'Negative Log Likelihood = {optim_results[i].fun}')\n",
    "    print(f'Temperature = {optim_results[i].x[0]}')\n",
    "    print(f'Optim. result: {optim_results[i].message} \\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
